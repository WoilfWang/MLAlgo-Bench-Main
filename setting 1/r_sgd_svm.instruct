Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the svm regressor based on the sgd algorithm with python, numpy and scipy. It uses the Gaussian kernel function. 
SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).

The core idea of SVM regression is to find a function \( f(x) \) that deviates from the actual target values \( y \) by a value no greater than \(\epsilon\) for each training point, while also being as flat as possible. This is achieved by minimizing the following objective function:

\[ 
\frac{1}{2} \| w \|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*)
\]

where:
- \( \| w \|^2 \) is the regularization term that ensures flatness.
- \( C \) is a penalty parameter that controls the trade-off between the flatness of \( f(x) \) and the amount up to which deviations larger than \(\epsilon\) are tolerated.
- \( \xi_i \) and \( \xi_i^* \) are slack variables that allow for deviations larger than \(\epsilon\).

### Gaussian Kernel

The Gaussian kernel, also known as the Radial Basis Function (RBF) kernel, is used to transform the input space into a higher-dimensional space where a linear separation is possible. The Gaussian kernel is defined as:

\[ 
K(x_i, x_j) = \exp\left(-\frac{\| x_i - x_j \|^2}{2\sigma^2}\right)
\]

where:
- \( \| x_i - x_j \|^2 \) is the squared Euclidean distance between two data points.
- \(\sigma\) is a parameter that defines the width of the Gaussian function.

### Algorithmic Flow with SGD

1. **Initialization**: Start with initial weights \( w \) and bias \( b \), often set to zero. Choose a learning rate \(\eta\) and the parameter \(\epsilon\).

2. **Kernel Transformation**: Transform the input data using the Gaussian kernel to map it into a higher-dimensional space.

3. **Iterative Optimization**:
   - For each training sample \( (x_i, y_i) \), compute the prediction:
     \[
     f(x_i) = \sum_{j=1}^{n} \alpha_j K(x_j, x_i) + b
     \]
   - Calculate the loss using the \(\epsilon\)-insensitive loss function:
     \[
     L(y_i, f(x_i)) = \max(0, |y_i - f(x_i)| - \epsilon)
     \]
   - Update the weights \( w \) and bias \( b \) using SGD:
     \[
     w = w - \eta \frac{\partial L}{\partial w}
     \]
     \[
     b = b - \eta \frac{\partial L}{\partial b}
     \]

4. **Convergence**: Repeat the optimization process until convergence, which is typically determined by a threshold on the change in the objective function or a maximum number of iterations.

5. **Prediction**: For a new input \( x \), predict the output using the learned weights and bias:
   \[
   \hat{y} = \sum_{j=1}^{n} \alpha_j K(x_j, x) + b
   \]


The module should be named GPTSGDSVM.  
The module must contain a fit function and a predict function.  
The fit function accepts X_train and y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted results for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.