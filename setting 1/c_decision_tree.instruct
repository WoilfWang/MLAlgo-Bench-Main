Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement decision classification tree model with python, numpy and scipy. It can handle multi-class classification problems.  
The information gain should use Gini impurity.

The decision tree is a popular machine learning algorithm used for both classification and regression tasks. It involves breaking down a dataset into smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches, each representing values for the attribute tested. Leaf nodes represent a classification or decision result. The paths from root to leaf represent classification rules.

### Principle of Decision Trees

The core idea is to select the best attribute to split the data set into smaller subsets to make the classes in each subset as pure as possible. The purity of a node can be measured using various metrics, one of which is Gini impurity.

### Gini Impurity

Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. For a set \( S \) containing \( K \) classes, the Gini impurity \( G(S) \) can be calculated as:

\[ G(S) = 1 - \sum_{i=1}^{K} p_i^2 \]

where \( p_i \) is the proportion of the class \( i \) elements in \( S \).

### Information Gain using Gini Impurity

When building a decision tree, we need to decide the best attribute to split the data. This is done by calculating the Information Gain for each attribute. The Information Gain is the change in Gini impurity before and after a split on an attribute. For a dataset \( S \) and an attribute \( A \) that splits \( S \) into subsets \( S_1, S_2, \ldots, S_n \), the Information Gain \( IG \) is given by:

\[ IG(S, A) = G(S) - \sum_{j=1}^{n} \frac{|S_j|}{|S|} G(S_j) \]

where \( |S_j| \) is the number of elements in subset \( S_j \), and \( |S| \) is the total number of elements in \( S \).

### Algorithmic Flow of Decision Tree (using Gini Impurity)

1. **Start at the root node** with the entire dataset.
2. **Select the best attribute** using Information Gain:
   - For each attribute, calculate the Information Gain.
   - Choose the attribute with the highest Information Gain.
3. **Split the data** into subsets using the selected attribute. Create a decision node based on the selected attribute.
4. **Repeat recursively** for each subset:
   - If all elements in the subset belong to the same class, create a leaf node.
   - If not, repeat the process of selecting the best attribute and splitting.
5. **Stop splitting** when:
   - All elements in the subset belong to the same class.
   - There are no more attributes to split on.
   - The depth of the tree reaches a predefined limit, or a minimum number of samples at a node is reached.
6. **Assign a class label** to each leaf node based on the majority class of the samples in the node.

The module should be named GPTDecisionClassificationTree.  
The init function should include the following parameters:
max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples;
min_samples_split: The minimum number of samples required to split an internal node;
min_samples_leaf: The minimum number of samples required to be at a leaf node.
The module must contain a fit function and a predict function.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted classes for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.