You should implement Vision Transformer using python, numpy, and pytorch from scratch. We provide the GPU that can be used for training or inference. 


Vision transformer applies a pure transformer to images without any convolution layers. They split the image into patches and apply a transformer on patch embeddings. Patch embeddings are generated by applying a simple linear transformation to the flattened pixel values of the patch. Then a standard transformer encoder is fed with the patch embeddings, along with a classification token [CLS] . The encoding on the [CLS] token is used to classify the image with an MLP.

When feeding the transformer with the patches, learned positional embeddings are added to the patch embeddings, because the patch embeddings do not have any information about where that patch is from. The positional embeddings are a set of vectors for each patch location that get trained with gradient descent along with other parameters.

ViTs perform well when they are pre-trained on large datasets. The paper suggests pre-training them with an MLP classification head and then using a single linear layer when fine-tuning. The paper beats SOTA with a ViT pre-trained on a 300 million image dataset. They also use higher resolution images during inference while keeping the patch size the same. The positional embeddings for new patch locations are calculated by interpolating learning positional embeddings.

You should implement patch embeddings, positional encodings, MLP Classification Head and Vision Transformer in sequence.

The patch embeddings should be named VitPatchEmbeddings. 
It splits the image into patches of equal size and do a linear transformation on the flattened pixels for each patch.
We implement the same thing through a convolution layer, because it's simpler to implement.
The init function of PatchEmbeddings should include the following parameters:
d_model: is the transformer embeddings size;
patch_size: is the size of the patch;
in_channels: is the number of channels in the input image (3 for rgb).
The module should include at least the following functions:
1. forward.
The function should include the following parameters:
x: is the input image of shape [batch_size, channels, height, width].
The return of the function is:
x: the patch embeddings.

The positional encodings should be named LearnedPositionalEmbeddings.
The init function of this module should include the following parameters:
d_model: is the transformer embeddings size;
max_len: is the maximum number of patches. The default value is 5000.
The module should include at least the following functions:
1. forward.
The function should include the following parameters:
x: is the patch embeddings of shape [patches, batch_size, d_model].

The MLP Classification Head should be named VitClassificationHead. 
This is the two layer MLP head to classify the image based on [CLS] token embedding.
The init function of the module should include the following parameters:
d_model: is the transformer embedding size;
n_hidden: is the size of the hidden layer;
n_classes: is the number of classes in the classification task.
The module should include at least the following functions:
1. forward.
THe function should include the following parameters:
x: is the transformer encoding for [CLS] token.
It returns:
x: the results of the mlp classification head.

The Vision Transformer should be named VisionTransformer.
The init function of this module should include the following parameters:
transformer_layer: is a copy of a single transformer layer. We make copies of it to make the transformer with n_layers; The input of transformer_layer is x and mask. As for mask, you need to set x=x and mask=None.
n_layers: is the number of transformer layers;
patch_emb: is the patch embeddings layer;
pos_emb: is the positional embeddings layer;
classification: is the classification head.
The module should include at least the following funcitons:
1. forward: forward propagation function. 
It should include the following parameters:
x: is the input image of shape [batch_size, channels, height, width].
It should return:
result: the result of the vision transformer.

You just need to implement the algorithm module; no need to provide corresponding examples