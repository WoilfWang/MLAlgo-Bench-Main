Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the tsne algorithm for dimensionality reduction with python, numpy and scipy.
t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results. 

The core idea of t-SNE is to convert high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities. It then attempts to find a lower-dimensional representation of the data that maintains these similarity relationships as closely as possible.

### Algorithmic Flow

1. **Compute Pairwise Affinities in High Dimensions:**
   - For each pair of data points \( x_i \) and \( x_j \), compute the conditional probability \( p_{j|i} \) that point \( x_j \) would be picked as a neighbor of point \( x_i \) if neighbors were picked in proportion to their probability density under a Gaussian centered at \( x_i \).
   - The conditional probability is given by:
     \[
     p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}
     \]
   - The bandwidth \( \sigma_i \) is determined using a binary search to achieve a fixed perplexity, which is a measure of the effective number of neighbors.

2. **Symmetrize the Affinities:**
   - The joint probability \( p_{ij} \) is symmetrized as:
     \[
     p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
     \]
   - Here, \( n \) is the number of data points.

3. **Compute Pairwise Affinities in Low Dimensions:**
   - In the low-dimensional space, use a Student's t-distribution with one degree of freedom (a Cauchy distribution) to compute the similarity \( q_{ij} \) between points \( y_i \) and \( y_j \):
     \[
     q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}
     \]

4. **Minimize the Kullback-Leibler Divergence:**
   - The goal is to minimize the Kullback-Leibler divergence between the joint probabilities \( P \) in the high-dimensional space and \( Q \) in the low-dimensional space:
     \[
     C = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
     \]
   - This is achieved using gradient descent. The gradient of the cost function with respect to a point \( y_i \) is:
     \[
     \frac{\partial C}{\partial y_i} = 4 \sum_{j} (p_{ij} - q_{ij})(y_i - y_j)(1 + \|y_i - y_j\|^2)^{-1}
     \]

5. **Iterative Optimization:**
   - Initialize the low-dimensional map randomly or using PCA.
   - Iteratively update the positions of the points in the low-dimensional space using the gradient descent method until convergence.

The module should be named GPTTSNE.
The init function should include the following parameters:
n_components: Dimension of the embedded space;
perplexity: The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results. The perplexity must be less than the number of samples.
The module must contain a fit_transform function, which is used for fitting data and performing dimensionality reduction transformations.
The fit_transform function accepts X as input and return reduced_X where
X: X is the features of the data, which is a numpy array and it's shape is [N, d]. N is the number of the train data and d is the dimension.
reduced_X: reduced_X is the reduced features after dimensionality reduction. The shape should be [N, low_d], where N is the num of the data and low_d is the reduced dimension.
You should just return the code for the module, don't return anything else.