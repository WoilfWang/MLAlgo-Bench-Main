Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement support vector machine for classification with python, numpy and scipy. It can handle multi-class classification problems. It uses the rbf kernel function.  

Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. The primary objective of SVM is to find the optimal hyperplane that separates data points of different classes with the maximum margin. When dealing with non-linearly separable data, SVM can be extended using kernel functions, such as the Radial Basis Function (RBF) kernel, to map data into a higher-dimensional space where a linear separation is possible.

### Principle of SVM

1. **Optimal Hyperplane**: In a binary classification problem, SVM aims to find a hyperplane that maximizes the margin between two classes. The margin is defined as the distance between the hyperplane and the nearest data point from either class, known as the support vectors.

2. **Maximizing the Margin**: The optimization problem can be formulated as:
   \[
   \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
   \]
   subject to the constraints:
   \[
   y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1, \quad \forall i
   \]
   where \(\mathbf{w}\) is the weight vector, \(b\) is the bias, \(\mathbf{x}_i\) are the input features, and \(y_i\) are the class labels.

3. **Dual Problem**: By introducing Lagrange multipliers, the problem can be transformed into its dual form:
   \[
   \max_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i \cdot \mathbf{x}_j)
   \]
   subject to:
   \[
   \sum_{i=1}^{n} \alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C
   \]
   where \(\alpha_i\) are the Lagrange multipliers and \(C\) is the regularization parameter.

### RBF Kernel

The RBF kernel, also known as the Gaussian kernel, is used to handle non-linear data by mapping it into a higher-dimensional space. The RBF kernel function is defined as:
\[
K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2\right)
\]
where \(\gamma\) is a parameter that defines the influence of a single training example.

### Algorithmic Flow

1. **Data Preparation**: Normalize or standardize the input data to ensure that each feature contributes equally to the distance calculations.

2. **Kernel Trick**: Use the RBF kernel to transform the input data into a higher-dimensional space.

3. **Solve the Dual Problem**: Use optimization techniques such as Sequential Minimal Optimization (SMO) to solve the dual problem and find the optimal \(\alpha_i\).

4. **Construct the Decision Function**: The decision function for classification is given by:
   \[
   f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{n} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right)
   \]

5. **Multi-Class Classification**: Extend the binary SVM to multi-class problems using strategies like one-vs-one or one-vs-all, where multiple binary classifiers are trained, and their results are combined.

The module should be named GPTSVM.  
The init function should include the following parameters:
C: Regularization parameter.
The module must contain a fit function and a predict function.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted classes for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.