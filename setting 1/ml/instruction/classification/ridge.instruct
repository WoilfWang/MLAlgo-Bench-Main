Implement the ridge classifier with python, numpy and scipy. Classifier using Ridge regression. It can handle multi-class classification problems. 
This classifier first converts the target values into ``{-1, 1}`` and then treats the problem as a regression task (multi-output regression in the multiclass case).

The Ridge Classifier can be understood through the lens of Ridge Regression, which is expressed as:

\[ \min_{\mathbf{w}} \left( \sum_{i=1}^{n} (y_i - \mathbf{w}^T \mathbf{x}_i)^2 + \alpha \|\mathbf{w}\|^2_2 \right) \]

Where:
- \( \mathbf{w} \) is the weight vector.
- \( \mathbf{x}_i \) is the feature vector for the \( i \)-th observation.
- \( y_i \) is the target value for the \( i \)-th observation.
- \( \alpha \) is the regularization parameter.
- \( \|\mathbf{w}\|^2_2 \) is the L2 norm of the weight vector, which acts as the regularization term.

### Algorithmic Flow

1. **Data Preparation**: Standardize the features to have zero mean and unit variance. This is crucial for Ridge Classifier as it ensures that the regularization term affects all features equally.

2. **Model Initialization**: Initialize the weight vector \( \mathbf{w} \) with small random values or zeros.

3. **Cost Function**: Define the cost function as the sum of the squared errors plus the regularization term:

   \[ J(\mathbf{w}) = \sum_{i=1}^{n} (y_i - \mathbf{w}^T \mathbf{x}_i)^2 + \alpha \|\mathbf{w}\|^2_2 \]

4. **Gradient Descent**: Update the weights iteratively using gradient descent:

   \[ \mathbf{w} = \mathbf{w} - \eta \left( -2 \sum_{i=1}^{n} (y_i - \mathbf{w}^T \mathbf{x}_i) \mathbf{x}_i + 2\alpha \mathbf{w} \right) \]

   Where \( \eta \) is the learning rate.

5. **Convergence Check**: Continue updating the weights until the change in the cost function is below a predefined threshold or a maximum number of iterations is reached.

6. **Prediction**: For classification, the decision boundary is determined by the sign of the linear combination of features:

   \[ \hat{y} = \text{sign}(\mathbf{w}^T \mathbf{x}) \]

   Where \(\hat{y}\) is the predicted class label.

The module should be named GPTRidge.  
The init function should include the following parameters:
alpha: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates.
The module must contain a fit function and a predict function.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted classes for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.