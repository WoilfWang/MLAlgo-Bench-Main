Implement linear discrimination classifier algorithm with python, numpy and scipy. It can handle multi-class classification problems.  

The main goal of LDA is to maximize the ratio of the between-class variance to the within-class variance, thereby ensuring maximum class separability. This is achieved by finding a linear transformation that projects the data into a space where the classes are as distinct as possible.

### Algorithmic Flow:

1. **Compute the Mean Vectors:**
   For each class, compute the mean vector:
   \[
   \mathbf{m}_i = \frac{1}{N_i} \sum_{\mathbf{x} \in \mathcal{D}_i} \mathbf{x}
   \]
   where \( \mathcal{D}_i \) is the set of samples in class \( i \), and \( N_i \) is the number of samples in class \( i \).

2. **Compute the Scatter Matrices:**
   - **Within-Class Scatter Matrix (\( \mathbf{S}_W \)):**
     \[
     \mathbf{S}_W = \sum_{i=1}^{c} \sum_{\mathbf{x} \in \mathcal{D}_i} (\mathbf{x} - \mathbf{m}_i)(\mathbf{x} - \mathbf{m}_i)^T
     \]
   - **Between-Class Scatter Matrix (\( \mathbf{S}_B \)):**
     \[
     \mathbf{S}_B = \sum_{i=1}^{c} N_i (\mathbf{m}_i - \mathbf{m})(\mathbf{m}_i - \mathbf{m})^T
     \]
     where \( \mathbf{m} \) is the overall mean vector of the dataset.

3. **Solve the Generalized Eigenvalue Problem:**
   The goal is to solve the following eigenvalue problem:
   \[
   \mathbf{S}_W^{-1} \mathbf{S}_B \mathbf{w} = \lambda \mathbf{w}
   \]
   where \( \lambda \) are the eigenvalues and \( \mathbf{w} \) are the eigenvectors.

4. **Select the Top Eigenvectors:**
   Choose the eigenvectors corresponding to the largest eigenvalues to form a matrix \( \mathbf{W} \). This matrix is used to transform the data into the new feature space.

5. **Project the Data:**
   Transform the original data \( \mathbf{X} \) using the matrix \( \mathbf{W} \):
   \[
   \mathbf{Y} = \mathbf{X} \mathbf{W}
   \]
   where \( \mathbf{Y} \) is the transformed dataset.

### Classification:

Once the data is projected, a simple classifier like a Gaussian Naive Bayes or a linear classifier can be used to classify the data in the reduced space.

### Mathematical Derivation:

The optimization problem can be expressed as maximizing the following objective function:
\[
J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}
\]
The solution involves finding the eigenvectors of \( \mathbf{S}_W^{-1} \mathbf{S}_B \) corresponding to the largest eigenvalues, which gives the directions of maximum separability.

The module should be named GPTLinearDiscrimination.  
The init function should include the following parameters:
num_classes: The num of the classes.
The module must contain a fit function and a predict function.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data, which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted classes for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.