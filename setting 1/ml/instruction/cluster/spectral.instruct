Implement the spectral clustering algorithm with python, numpy and scipy. 
Apply clustering to a projection of the normalized Laplacian. In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex, or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster, such as when clusters are nested circles on the 2D plane. If the affinity matrix is the adjacency matrix of a graph, this method can be used to find normalized graph cuts. When calling fit, an affinity matrix is constructed using either a kernel function such the Gaussian (aka RBF) kernel with Euclidean distance d(X, X): np.exp(-gamma * d(X,X) ** 2) or a k-nearest neighbors connectivity matrix. 

Spectral clustering is based on the idea of using the spectrum (eigenvalues and eigenvectors) of a similarity matrix of the data to reduce dimensionality and identify clusters. The key insight is that the eigenvectors of a graph Laplacian matrix can reveal the structure of the data in a way that is conducive to clustering.

### Algorithmic Flow

1. **Construct the Similarity Matrix**:
   - Given a set of data points \( X = \{x_1, x_2, \ldots, x_n\} \), construct a similarity matrix \( S \) where each entry \( S_{ij} \) represents the similarity between data points \( x_i \) and \( x_j \). A common choice is the Gaussian (RBF) kernel:
     \[
     S_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)
     \]
   - Alternatively, you can use a k-nearest neighbor approach to define \( S \).

2. **Construct the Laplacian Matrix**:
   - Compute the degree matrix \( D \), which is a diagonal matrix where \( D_{ii} = \sum_{j} S_{ij} \).
   - The unnormalized graph Laplacian \( L \) is defined as:
     \[
     L = D - S
     \]
   - Alternatively, you can use the normalized graph Laplacian:
     \[
     L_{\text{sym}} = I - D^{-1/2} S D^{-1/2}
     \]
     or
     \[
     L_{\text{rw}} = I - D^{-1} S
     \]

3. **Compute Eigenvectors**:
   - Compute the first \( k \) eigenvectors \( u_1, u_2, \ldots, u_k \) of the Laplacian matrix \( L \) (or \( L_{\text{sym}} \) or \( L_{\text{rw}} \)), corresponding to the smallest \( k \) eigenvalues.

4. **Form the Feature Matrix**:
   - Construct a matrix \( U \) where each column corresponds to one of the \( k \) eigenvectors. Each row of \( U \) can be considered as a new representation of the original data points in a reduced \( k \)-dimensional space.

5. **Cluster the Data**:
   - Apply a standard clustering algorithm, such as k-means, to the rows of the matrix \( U \). The resulting clusters correspond to the clusters in the original data space.

### Mathematical Derivation

The spectral clustering algorithm is rooted in graph theory and linear algebra. The Laplacian matrix \( L \) captures the connectivity of the graph formed by the data points, and its eigenvectors provide a way to partition the graph into disjoint sets. The smallest eigenvalues and their corresponding eigenvectors are used because they capture the most significant structure of the graph.

The normalized Laplacians \( L_{\text{sym}} \) and \( L_{\text{rw}} \) are often preferred because they provide better theoretical guarantees and are more robust to variations in the degree of the nodes in the graph.


The module should be named GPTSpectralClustering.  
The init function should include the following parameters:
n_clusters: The dimension of the projection subspace.
The module must contain a fit_predict function.  
The fit_predict function accepts X as input and return labels where  
X: X is the features of the data, which is a numpy array and it's shape is [N, d]. N is the number of the train data and d is the dimension.  
labels: A numpy array of shape (n_samples,) containing the index of the cluster each sample belongs to.  
You should just return the code for the module, don't return anything else.