Implement the linear regressor with python, numpy and scipy.  

The main objective of linear regression is to determine the linear relationship between the dependent variable \( y \) and the independent variable(s) \( X \). The simplest form is the simple linear regression, which involves one independent variable. The relationship is modeled by the equation:

\[ y = \beta_0 + \beta_1 x + \epsilon \]

- \( y \) is the dependent variable.
- \( x \) is the independent variable.
- \( \beta_0 \) is the y-intercept of the regression line.
- \( \beta_1 \) is the slope of the regression line.
- \( \epsilon \) is the error term, representing the difference between the observed and predicted values.

### Algorithmic Flow

1. **Data Collection and Preparation**: Gather the dataset containing the dependent and independent variables. Clean and preprocess the data to handle missing values and outliers.

2. **Model Specification**: Define the linear model. For multiple linear regression, the model extends to:

   \[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon \]

3. **Parameter Estimation**: Use the method of least squares to estimate the parameters \( \beta_0, \beta_1, \ldots, \beta_n \). The goal is to minimize the sum of squared residuals (differences between observed and predicted values):

   \[ \text{Minimize } \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 \]

   where \( \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in} \).

4. **Normal Equations**: The least squares estimates can be derived using the normal equation:

   \[ \beta = (X^T X)^{-1} X^T y \]

   Here, \( X \) is the matrix of input features, \( y \) is the vector of observed values, and \( \beta \) is the vector of coefficients.

5. **Model Evaluation**: Assess the model's performance using metrics such as R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics help determine how well the model fits the data.

6. **Prediction**: Use the estimated coefficients to make predictions on new data:

   \[ \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n \]

The module should be named GPTLinearRegressor.  
The module must contain a fit function and a predict function. The fit function is used to fit the training data, and the predict function is used to predict the labels for the given features.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted labels for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.