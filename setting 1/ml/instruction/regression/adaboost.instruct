Implement a adaboost regressor for regression with python, numpy and scipy. 
An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.

### Principle of AdaBoost Regressor

1. **Weak Learners**: The base learners in AdaBoost are typically simple models, such as decision tree regressors with a single split, also known as decision stumps. These models are weak in the sense that they perform slightly better than random guessing.

2. **Iterative Improvement**: AdaBoost builds the model in a sequential manner. Each new model is trained to correct the errors made by the previous ensemble of models.

3. **Weighted Data Points**: The algorithm assigns weights to each data point. Initially, all data points have equal weights. After each iteration, the weights are adjusted so that more emphasis is placed on the data points that were predicted poorly by the previous models.

### Algorithmic Flow of AdaBoost Regressor

1. **Initialize Weights**: Start with equal weights for all training data points. If there are \( N \) data points, each weight \( w_i \) is initialized to \( \frac{1}{N} \).

2. **Iterate Over Weak Learners**:
   - For each iteration \( t \) (from 1 to \( T \), where \( T \) is the total number of iterations):
     1. **Train a Weak Learner**: Fit a decision tree regressor to the training data, using the current weights.
     2. **Compute Error**: Calculate the weighted error \( \epsilon_t \) of the model:
        \[
        \epsilon_t = \frac{\sum_{i=1}^{N} w_i \cdot |y_i - \hat{y}_i|}{\sum_{i=1}^{N} w_i}
        \]
        where \( y_i \) is the true value and \( \hat{y}_i \) is the predicted value.
     3. **Compute Model Weight**: Calculate the weight \( \alpha_t \) of the model:
        \[
        \alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)
        \]
     4. **Update Weights**: Update the weights of the data points:
        \[
        w_i \leftarrow w_i \cdot \exp(\alpha_t \cdot |y_i - \hat{y}_i|)
        \]
        Normalize the weights so that they sum to 1.

3. **Aggregate Models**: The final prediction is a weighted sum of the predictions from all the weak learners:
   \[
   \hat{y} = \sum_{t=1}^{T} \alpha_t \cdot \hat{y}_t
   \]
   where \( \hat{y}_t \) is the prediction from the \( t \)-th weak learner.

The base regressor is decision tree regressor, which you should implement from scratch. The max depth should be set to 3. 
The module should be named GPTAdaboostRegressor.  
The init function should include the following parameters:
n_estimators: The maximum number of estimators at which boosting is terminated;
learning_rate: Weight applied to each regressor at each boosting iteration.
The module must contain a fit function and a predict function.  
The fit function accepts X_train and y_train and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted results for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.