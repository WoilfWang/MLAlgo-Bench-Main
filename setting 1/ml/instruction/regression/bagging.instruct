Implement the Bagging regressor with python, numpy and scipy. 
A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]. If samples are drawn with replacement, then the method is known as Bagging [2]. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4].


The core idea of Bagging is to create multiple subsets of the original dataset through bootstrapping (random sampling with replacement) and train a base model on each subset. The predictions from these models are then aggregated to produce a final prediction. For regression tasks, this aggregation is typically done by averaging the predictions.

### Algorithmic Flow of Bagging Regressor

1. **Bootstrap Sampling**: From the original dataset \( D \) with \( n \) samples, create \( B \) bootstrap samples \( D_1, D_2, \ldots, D_B \). Each bootstrap sample is created by randomly selecting \( n \) samples from \( D \) with replacement.

2. **Model Training**: For each bootstrap sample \( D_i \), train a base regressor \( f_i(x) \). This could be any regression model, but decision trees are commonly used due to their high variance, which Bagging effectively reduces.

3. **Prediction Aggregation**: For a new input \( x \), obtain predictions from each of the \( B \) trained models: \( f_1(x), f_2(x), \ldots, f_B(x) \). The final prediction \( \hat{f}(x) \) is the average of these predictions:
   \[
   \hat{f}(x) = \frac{1}{B} \sum_{i=1}^{B} f_i(x)
   \]

### Mathematical Explanation

- **Variance Reduction**: Bagging reduces the variance of the model by averaging multiple predictions. The variance of the average of \( B \) independent models is reduced by a factor of \( B \):
  \[
  \text{Var}(\hat{f}(x)) = \frac{1}{B} \text{Var}(f(x))
  \]
  assuming the models are independent and identically distributed.

The base regressor is decision tree regressor, which you should implement from scratch. 
The module should be named GPTBagging.  
The init function should include the following parameters:
n_estimators: The number of base estimators in the ensemble;
max_samples: The number of samples to draw from X to train each base estimator;
max_features: The number of features to draw from X to train each base estimator.
The module must contain a fit function and a predict function.  
The fit function accepts X_train and y_train and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted results for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.