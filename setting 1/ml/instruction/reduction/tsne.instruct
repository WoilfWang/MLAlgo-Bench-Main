Implement the tsne algorithm for dimensionality reduction with python, numpy and scipy.
t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results. 

The core idea of t-SNE is to convert high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities. It then attempts to find a lower-dimensional representation of the data that maintains these similarity relationships as closely as possible.

### Algorithmic Flow

1. **Compute Pairwise Affinities in High Dimensions:**
   - For each pair of data points \( x_i \) and \( x_j \), compute the conditional probability \( p_{j|i} \) that point \( x_j \) would be picked as a neighbor of point \( x_i \) if neighbors were picked in proportion to their probability density under a Gaussian centered at \( x_i \).
   - The conditional probability is given by:
     \[
     p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}
     \]
   - The bandwidth \( \sigma_i \) is determined using a binary search to achieve a fixed perplexity, which is a measure of the effective number of neighbors.

2. **Symmetrize the Affinities:**
   - The joint probability \( p_{ij} \) is symmetrized as:
     \[
     p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
     \]
   - Here, \( n \) is the number of data points.

3. **Compute Pairwise Affinities in Low Dimensions:**
   - In the low-dimensional space, use a Student's t-distribution with one degree of freedom (a Cauchy distribution) to compute the similarity \( q_{ij} \) between points \( y_i \) and \( y_j \):
     \[
     q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}
     \]

4. **Minimize the Kullback-Leibler Divergence:**
   - The goal is to minimize the Kullback-Leibler divergence between the joint probabilities \( P \) in the high-dimensional space and \( Q \) in the low-dimensional space:
     \[
     C = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
     \]
   - This is achieved using gradient descent. The gradient of the cost function with respect to a point \( y_i \) is:
     \[
     \frac{\partial C}{\partial y_i} = 4 \sum_{j} (p_{ij} - q_{ij})(y_i - y_j)(1 + \|y_i - y_j\|^2)^{-1}
     \]

5. **Iterative Optimization:**
   - Initialize the low-dimensional map randomly or using PCA.
   - Iteratively update the positions of the points in the low-dimensional space using the gradient descent method until convergence.

The module should be named GPTTSNE.
The init function should include the following parameters:
n_components: Dimension of the embedded space;
perplexity: The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results. The perplexity must be less than the number of samples.
The module must contain a fit_transform function, which is used for fitting data and performing dimensionality reduction transformations.
The fit_transform function accepts X as input and return reduced_X where
X: X is the features of the data, which is a numpy array and it's shape is [N, d]. N is the number of the train data and d is the dimension.
reduced_X: reduced_X is the reduced features after dimensionality reduction. The shape should be [N, low_d], where N is the num of the data and low_d is the reduced dimension.
You should just return the code for the module, don't return anything else.