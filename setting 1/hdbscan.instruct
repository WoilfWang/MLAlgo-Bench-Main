Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the HDBSCAN clustering algorithm with python, numpy and scipy. 
HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications with Noise. Performs DBSCAN over varying epsilon values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection.

### Step 1: Transform the Space Based on Density
HDBSCAN starts by transforming the space according to density, so that distance in the transformed space reflects the density-based "closeness". This is achieved by computing the "mutual reachability distance" between points.

#### Mutual Reachability Distance
Given a minimum cluster size parameter `min_samples`, the mutual reachability distance between two points \( x \) and \( y \) is defined as:
\[ d_{\text{mreach}}(x, y) = \max\{ \text{core}_k(x), \text{core}_k(y), d(x, y) \} \]
where:
- \( d(x, y) \) is the Euclidean distance between \( x \) and \( y \),
- \( \text{core}_k(x) \) is the core distance for \( x \), defined as the distance from \( x \) to its \( k \)-th nearest neighbor (where \( k = \text{min\_samples} \)).

### Step 2: Build the Minimum Spanning Tree (MST)
Using the mutual reachability distances, HDBSCAN constructs a minimum spanning tree (MST) of the dataset. This tree connects all points in such a way that the total sum of the edge weights (mutual reachability distances) is minimized.

### Step 3: Construct the Hierarchy
From the MST, HDBSCAN builds a hierarchical tree of clusters. This is done by removing the longest edge, creating two clusters, then continuing to remove edges from longest to shortest, thereby splitting clusters progressively until each data point is its own cluster.

### Step 4: Condense the Tree
The full hierarchical tree can be very large and complex. HDBSCAN condenses this tree by summarizing and collapsing clusters that do not show significant persistence over scale; essentially, it removes the "noise" or less dense points that do not contribute to the core structure of the data.

### Step 5: Extract the Clusters
Finally, clusters are extracted from the condensed tree based on stability. A cluster's stability is calculated as the sum of excess of density (over the minimum cluster size threshold) multiplied by the distance over which it persists as the dominant cluster. The algorithm selects clusters with the greatest stability.

#### Stability Calculation
For a cluster \( C \) persisting over a range \([e_{\text{min}}, e_{\text{max}}]\), the stability is:
\[ \text{Stability}(C) = \sum_{x \in C} (e_{\text{max}} - e_{\text{min}}) \cdot (\text{size}(C) - \text{min\_samples}) \]
where \( \text{size}(C) \) is the number of points in \( C \).

The module should be named GPTHDBSCAN.  
The init function should include the following parameters:
min_cluster_size: The minimum number of samples in a group for that group to be considered a cluster; groupings smaller than this size will be left as noise.
The module must contain a fit_predict function.  
The fit_predict function accepts X as input and return labels where  
X: X is the features of the data, which is a numpy array and it's shape is [N, d]. N is the number of the train data and d is the dimension.  
labels: A numpy array of shape (n_samples,) containing the index of the cluster each sample belongs to.  
You should just return the code for the module, don't return anything else.