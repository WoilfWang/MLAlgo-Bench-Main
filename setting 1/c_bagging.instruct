Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the bagging classifier with python, numpy and scipy from scratch. It can handle multi-class classification problems. 

A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]. If samples are drawn with replacement, then the method is known as Bagging [2]. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4].

Algorithmic Flow

	1.	Bootstrap Sampling: From the original dataset of size  N , create  M  new training sets. Each new training set is formed by randomly selecting  N  samples from the original dataset with replacement. This means each new training set may have duplicate records and some of the original records may be missing.
	2.	Train Base Learners: Train a separate classifier (base learner) on each of the  M  bootstrapped datasets. These classifiers can be all of the same type or can differ from each other.
	3.	Aggregation: After training, aggregate the predictions of the  M  classifiers. The aggregation method depends on the type of problem:
	•	Classification: The final output can be decided by majority voting (hard voting) or by averaging the probabilistic/soft outputs (soft voting) from each classifier.
	•	Regression: The final prediction is typically the average of the predictions from each classifier.

Mathematical Formulation

Assume we have a dataset  D  of size  N . For bagging, we generate  M  new training sets \( D_1, D_2, \dots, D_M \), each of size  N , by sampling from  D  with replacement. Let  h(x; D_i)  be the prediction of the classifier trained on the  i -th bootstrapped dataset  D_i .

	•	Classification: If  h_i(x)  denotes the output of the  i -th classifier, the bagged classifier  H(x)  is given by:

H(x) = \text{mode} \{h_1(x), h_2(x), \ldots, h_M(x)\}

where  \text{mode}  denotes the most frequent label among the  M  predictions.
	•	Regression: The aggregated output is the average of the predictions:

H(x) = \frac{1}{M} \sum_{i=1}^M h_i(x)

The base classifier is decision tree. You should first implement the decision tree from scratch.  
The module should be named GPTBagging.  
The init function should include the following parameters:
n_estimators: The number of base estimators in the ensemble;
max_samples: The number of samples to draw from X to train each base estimator;
max_features: The number of features to draw from X to train each base estimator.
The module must contain a fit function and a predict function.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted classes for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.