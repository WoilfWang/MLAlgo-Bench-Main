Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the Mini-batch Sparse Principal Components Analysis algorithm for dimensionality reduction with python, numpy and scipy. 
Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.

The main goal of Sparse PCA is to find principal components that are linear combinations of a small number of original variables, thus introducing sparsity. This is achieved by adding a sparsity-inducing penalty to the PCA optimization problem. The mini-batch approach processes data in small batches, which reduces memory usage and can speed up computation.

### Algorithmic Flow

1. **Initialization**: 
   - Start with a dataset \( X \in \mathbb{R}^{n \times p} \), where \( n \) is the number of samples and \( p \) is the number of features.
   - Initialize the sparse loading matrix \( W \in \mathbb{R}^{p \times k} \), where \( k \) is the number of components to extract.

2. **Mini-batch Selection**:
   - Randomly select a mini-batch \( X_{\text{batch}} \) from the dataset. The size of the mini-batch is typically much smaller than \( n \).

3. **Sparse PCA Objective**:
   - The objective function for Sparse PCA can be formulated as:
     \[
     \min_{W} \|X_{\text{batch}} - X_{\text{batch}}WW^T\|_F^2 + \lambda \|W\|_1
     \]
   - Here, \( \| \cdot \|_F \) denotes the Frobenius norm, and \( \|W\|_1 \) is the \( \ell_1 \)-norm of \( W \), which promotes sparsity. The parameter \( \lambda \) controls the trade-off between reconstruction error and sparsity.

4. **Optimization**:
   - Use an iterative optimization algorithm such as coordinate descent or proximal gradient descent to solve the above objective for the mini-batch. These methods are well-suited for handling the non-smooth \( \ell_1 \)-norm penalty.

5. **Update**:
   - Update the loading matrix \( W \) based on the optimization results from the mini-batch.

6. **Repeat**:
   - Repeat the mini-batch selection, optimization, and update steps until convergence or for a fixed number of iterations.

7. **Aggregation**:
   - After processing all mini-batches, aggregate the results to form the final sparse principal components.

### Mathematical Derivations

- **Sparsity Induction**: The \( \ell_1 \)-norm penalty \( \lambda \|W\|_1 \) is crucial for inducing sparsity in the loading matrix \( W \). This penalty encourages many entries of \( W \) to be zero, resulting in sparse principal components.
  
- **Optimization Techniques**: Proximal gradient descent is often used due to its ability to handle non-differentiable terms like the \( \ell_1 \)-norm. The proximal operator for the \( \ell_1 \)-norm is the soft-thresholding function, which is applied to update \( W \) during each iteration.

The module should be named GPTMiniBatchSparsePCA.
The init function should include the following parameters:
n_components: Number of sparse atoms to extract;
alpha: Sparsity controlling parameter. Higher values lead to sparser components;
ridge_alpha: Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method;
batch_size: The number of features to take in each mini batch.
The module must contain a fit_transform function, which is used for fitting data and performing dimensionality reduction transformations.
The fit_transform function accepts X as input and return reduced_X where
X: X is the features of the data, which is a numpy array and it's shape is [N, d]. N is the number of the train data and d is the dimension.
reduced_X: reduced_X is the reduced features after dimensionality reduction. The shape should be [N, low_d], where N is the num of the data and low_d is the reduced dimension.
You should just return the code for the module, don't return anything else.