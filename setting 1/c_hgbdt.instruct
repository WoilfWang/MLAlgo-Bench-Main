Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the Histogram-based Gradient Boosting Classification Tree with python, numpy and scipy. It can handle multi-class classification problems. 
This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently. If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.   

### Algorithmic Flow

1. **Initialization**:
   - Start with an initial model, \( F_0(x) \), which is typically a constant model. For classification, this could be the log-odds of the target classes.

2. **Iterative Boosting**:
   - For each iteration \( m = 1, 2, \ldots, M \):
     1. **Compute Pseudo-Residuals**:
        - For each sample \( i \), compute the pseudo-residuals:
          \[
          r_{i}^{(m)} = -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \bigg|_{F(x) = F_{m-1}(x)}
          \]
        - Here, \( L(y_i, F(x_i)) \) is the loss function, such as the logistic loss for binary classification.

     2. **Histogram Binning**:
        - Discretize the continuous features into bins to create histograms. This reduces the number of potential split points, making the algorithm more efficient.

     3. **Fit a CART to Pseudo-Residuals**:
        - Train a decision tree \( h_m(x) \) using the pseudo-residuals \( r_{i}^{(m)} \) as the target. The tree is built using the histogram-binned features to find the best splits.

     4. **Update the Model**:
        - Update the model by adding the new tree, scaled by a learning rate \( \nu \):
          \[
          F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
          \]
        - The learning rate \( \nu \) is a hyperparameter that controls the contribution of each tree.

3. **Final Prediction**:
   - After \( M \) iterations, the final model \( F_M(x) \) is used to make predictions. For classification, the output is typically transformed using a logistic function to produce class probabilities.

### Mathematical Derivations

- **Loss Function**: For binary classification, the logistic loss is often used:
  \[
  L(y, F(x)) = \log(1 + \exp(-y \cdot F(x)))
  \]
  where \( y \in \{-1, 1\} \).

- **Gradient Calculation**: The gradient of the logistic loss with respect to the model's prediction is:
  \[
  \frac{\partial L(y, F(x))}{\partial F(x)} = -\frac{y}{1 + \exp(y \cdot F(x))}
  \]

- **Tree Fitting**: The decision tree \( h_m(x) \) is fitted to minimize the squared error of the pseudo-residuals:
  \[
  \sum_{i=1}^{n} (r_{i}^{(m)} - h_m(x_i))^2
  \]

The module should be named GPTHistGradientBoostingClassifier.  
The base learners are CART. You should first implement CART from scratch. The loss should be log loss.
The init function should include the following parameters:
learning_rate: Learning rate shrinks the contribution of each tree by learning_rate;
max_iter: The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, n_classes trees per iteration are built;
max_leaf_nodes: The maximum number of leaves for each tree. Must be strictly greater than 1. If None, there is no maximum limit;
max_depth: The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isnâ€™t constrained by default;
min_samples_leaf: The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.
The module must contain a fit function and a predict function. The fit function is used to fit the training data, and the predict function is used to predict the labels for the given features.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted labels for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.