Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the Incremental principal components analysis algorithm for dimensionality reduction with python, numpy and scipy. 
Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD. Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA, and allows sparse input. This algorithm has constant memory complexity, on the order of ``batch_size * n_features``, enabling use of np.memmap files without loading the entire file into memory. For sparse matrices, the input is converted to dense in batches (in order to be able to subtract the mean) which avoids storing the entire dense matrix at any one time. The computational overhead of each SVD is ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples remain in memory at a time. There will be ``n_samples / batch_size`` SVD computations to get the principal components, versus 1 large SVD of complexity ``O(n_samples * n_features ** 2)`` for PCA.

The core idea of IPCA is to update the principal components incrementally as new data becomes available, rather than recalculating them from scratch. This is achieved by maintaining a running estimate of the principal components and updating this estimate with each new data point or batch of data.

### Algorithmic Flow

1. **Initialization**: Start with an initial estimate of the mean vector \(\mu_0\) and the covariance matrix \(\Sigma_0\). If no prior data is available, these can be initialized to zero or based on a small initial dataset.

2. **Data Processing**: For each new data point \(x_t\) (or batch of data), update the mean and covariance estimates:
   - Update the mean:
     \[
     \mu_t = \mu_{t-1} + \frac{1}{t}(x_t - \mu_{t-1})
     \]
   - Update the covariance matrix:
     \[
     \Sigma_t = \Sigma_{t-1} + \frac{1}{t}((x_t - \mu_{t-1})(x_t - \mu_{t-1})^T - \Sigma_{t-1})
     \]

3. **Eigen Decomposition**: Perform an eigen decomposition on the updated covariance matrix \(\Sigma_t\) to obtain the eigenvectors and eigenvalues. The eigenvectors corresponding to the largest eigenvalues are the principal components.

4. **Dimensionality Reduction**: Project the data onto the principal components to reduce dimensionality. This can be done by multiplying the data by the matrix of selected eigenvectors.

5. **Repeat**: Continue processing new data points, updating the mean, covariance, and principal components incrementally.

### Mathematical Derivations

The update rules for the mean and covariance matrix are derived from the properties of expectation and variance. The mean update is a simple moving average, while the covariance update is derived from the formula for the sample covariance matrix, adjusted to account for the incremental nature of the data.

The eigen decomposition step is computationally intensive, but since the covariance matrix is updated incrementally, the changes are often small, allowing for efficient updates using techniques like the power iteration method or other iterative eigenvalue algorithms.

The module should be named GPTIncrementalPCA.
The init function should include the following parameters:
n_components: Number of components to keep;
batch_size: The number of samples to use for each batch. Only used when calling fit. If batch_size is None, then batch_size is inferred from the data and set to 5 * n_features, to provide a balance between approximation accuracy and memory consumption.
The module must contain a fit_transform function, which is used for fitting data and performing dimensionality reduction transformations.
The fit_transform function accepts X as input and return reduced_X where
X: X is the features of the data, which is a numpy array and it's shape is [N, d]. N is the number of the train data and d is the dimension.
reduced_X: reduced_X is the reduced features after dimensionality reduction. The shape should be [N, low_d], where N is the num of the data and low_d is the reduced dimension.
You should just return the code for the module, don't return anything else.