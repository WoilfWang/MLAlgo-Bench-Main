Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement linear discrimination classifier algorithm with python, numpy and scipy. It can handle multi-class classification problems.  

The main goal of LDA is to maximize the ratio of the between-class variance to the within-class variance, thereby ensuring maximum class separability. This is achieved by finding a linear transformation that projects the data into a space where the classes are as distinct as possible.

### Algorithmic Flow:

1. **Compute the Mean Vectors:**
   For each class, compute the mean vector:
   \[
   \mathbf{m}_i = \frac{1}{N_i} \sum_{\mathbf{x} \in \mathcal{D}_i} \mathbf{x}
   \]
   where \( \mathcal{D}_i \) is the set of samples in class \( i \), and \( N_i \) is the number of samples in class \( i \).

2. **Compute the Scatter Matrices:**
   - **Within-Class Scatter Matrix (\( \mathbf{S}_W \)):**
     \[
     \mathbf{S}_W = \sum_{i=1}^{c} \sum_{\mathbf{x} \in \mathcal{D}_i} (\mathbf{x} - \mathbf{m}_i)(\mathbf{x} - \mathbf{m}_i)^T
     \]
   - **Between-Class Scatter Matrix (\( \mathbf{S}_B \)):**
     \[
     \mathbf{S}_B = \sum_{i=1}^{c} N_i (\mathbf{m}_i - \mathbf{m})(\mathbf{m}_i - \mathbf{m})^T
     \]
     where \( \mathbf{m} \) is the overall mean vector of the dataset.

3. **Solve the Generalized Eigenvalue Problem:**
   The goal is to solve the following eigenvalue problem:
   \[
   \mathbf{S}_W^{-1} \mathbf{S}_B \mathbf{w} = \lambda \mathbf{w}
   \]
   where \( \lambda \) are the eigenvalues and \( \mathbf{w} \) are the eigenvectors.

4. **Select the Top Eigenvectors:**
   Choose the eigenvectors corresponding to the largest eigenvalues to form a matrix \( \mathbf{W} \). This matrix is used to transform the data into the new feature space.

5. **Project the Data:**
   Transform the original data \( \mathbf{X} \) using the matrix \( \mathbf{W} \):
   \[
   \mathbf{Y} = \mathbf{X} \mathbf{W}
   \]
   where \( \mathbf{Y} \) is the transformed dataset.

### Classification:

Once the data is projected, a simple classifier like a Gaussian Naive Bayes or a linear classifier can be used to classify the data in the reduced space.

### Mathematical Derivation:

The optimization problem can be expressed as maximizing the following objective function:
\[
J(\mathbf{w}) = \frac{\mathbf{w}^T \mathbf{S}_B \mathbf{w}}{\mathbf{w}^T \mathbf{S}_W \mathbf{w}}
\]
The solution involves finding the eigenvectors of \( \mathbf{S}_W^{-1} \mathbf{S}_B \) corresponding to the largest eigenvalues, which gives the directions of maximum separability.

The module should be named GPTLinearDiscrimination.  
The init function should include the following parameters:
num_classes: The num of the classes.
The module must contain a fit function and a predict function.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data, which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted classes for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.