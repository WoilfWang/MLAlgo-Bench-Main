Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the gredient boosting decision tree for classification with python, numpy and scipy. It can handle multi-class classification problems.  

Gradient Boosting Decision Trees (GBDT) is a powerful ensemble learning technique used for classification and regression tasks. It combines the predictions of multiple decision trees, specifically Classification and Regression Trees (CART), to improve the overall model performance. The principle behind GBDT is to build models sequentially, where each new model attempts to correct the errors made by the previous models.

### Algorithmic Flow

1. **Initialize the Model**: Start with an initial model, typically a constant value. For classification, this could be the log-odds of the target classes.

   \[
   F_0(x) = \arg\min_{\gamma} \sum_{i=1}^{N} L(y_i, \gamma)
   \]

   where \( L \) is the loss function, \( y_i \) are the true labels, and \( \gamma \) is a constant.

2. **Iterative Boosting**: For each iteration \( m = 1, 2, \ldots, M \):

   a. **Compute the Pseudo-Residuals**: Calculate the negative gradient of the loss function with respect to the current model's predictions. This serves as the target for the new tree.

   \[
   r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x) = F_{m-1}(x)}
   \]

   b. **Fit a CART to the Pseudo-Residuals**: Train a decision tree \( h_m(x) \) to predict the pseudo-residuals.

   c. **Compute the Step Size**: Determine the optimal step size (learning rate) by minimizing the loss function along the direction of the new tree.

   \[
   \gamma_m = \arg\min_{\gamma} \sum_{i=1}^{N} L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i))
   \]

   d. **Update the Model**: Update the model by adding the scaled predictions of the new tree.

   \[
   F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x)
   \]

   where \( \nu \) is the learning rate, a hyperparameter that controls the contribution of each tree.

3. **Final Prediction**: After \( M \) iterations, the final model is used for prediction. For classification, the output is typically transformed using a logistic function to produce probabilities.

   \[
   \hat{y} = \text{sigmoid}(F_M(x)) = \frac{1}{1 + e^{-F_M(x)}}
   \]


The module should be named GPTGradientBoostDecisionTree.  
The base learners are CART. You should first implement CART from scratch. 
The init function should include the following parameters:
learning_rate: Learning rate shrinks the contribution of each tree by learning_rate;
n_estimators: The number of boosting stages to perform;
subsample: The fraction of samples to be used for fitting the individual base learners;
max_depth: Maximum depth of the individual regression estimators.
The module must contain a fit function and a predict function.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted classes for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.