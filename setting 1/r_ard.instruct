Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the Bayesian ARD regression with python, numpy and scipy. 
Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)  

The core idea behind Bayesian ARD regression is to place a prior distribution over the regression coefficients that encourages sparsity. This is achieved by using a Gaussian prior with a separate variance parameter for each coefficient. The variance parameters themselves are governed by hyperpriors, which are typically inverse-Gamma distributions. This hierarchical Bayesian model allows the data to inform which features are relevant by shrinking the coefficients of irrelevant features towards zero.

### Algorithmic Flow

1. **Model Specification:**
   - Assume a linear model: \( y = X\beta + \epsilon \), where \( y \) is the response vector, \( X \) is the design matrix, \( \beta \) is the vector of coefficients, and \( \epsilon \) is the noise term, typically assumed to be Gaussian: \( \epsilon \sim \mathcal{N}(0, \sigma^2 I) \).

2. **Prior Distribution:**
   - Place a Gaussian prior on the coefficients: \( \beta_j \sim \mathcal{N}(0, \alpha_j^{-1}) \), where \( \alpha_j \) is the precision (inverse variance) of the \( j \)-th coefficient.
   - Use a hyperprior for the precision parameters: \( \alpha_j \sim \text{Gamma}(a, b) \).

3. **Posterior Inference:**
   - The goal is to compute the posterior distribution of the coefficients \( p(\beta | X, y) \). This involves integrating over the precision parameters \( \alpha \), which is typically intractable.
   - Use approximate inference techniques such as Variational Bayes or Expectation Maximization (EM) to estimate the posterior distribution.

4. **Variational Bayes Approach:**
   - Assume a factorized form for the posterior: \( q(\beta, \alpha) = q(\beta)q(\alpha) \).
   - Iteratively update the variational parameters by minimizing the Kullback-Leibler divergence between the true posterior and the variational approximation.
   - Update rules involve computing expectations of the sufficient statistics under the current variational distribution.

5. **Feature Relevance:**
   - The ARD mechanism works by adjusting the precision parameters \( \alpha_j \). Features with large \( \alpha_j \) have their corresponding coefficients \( \beta_j \) shrunk towards zero, effectively deeming them irrelevant.
   - The model automatically prunes irrelevant features by driving their precision parameters to high values.

### Mathematical Derivations

- **Likelihood:** \( p(y | X, \beta, \sigma^2) = \mathcal{N}(y | X\beta, \sigma^2 I) \)
- **Prior on Coefficients:** \( p(\beta | \alpha) = \prod_{j=1}^{p} \mathcal{N}(\beta_j | 0, \alpha_j^{-1}) \)
- **Hyperprior on Precision:** \( p(\alpha) = \prod_{j=1}^{p} \text{Gamma}(\alpha_j | a, b) \)

The posterior distribution is given by:

\[ p(\beta, \alpha | X, y) \propto p(y | X, \beta, \sigma^2) p(\beta | \alpha) p(\alpha) \]

The module should be named GPTARDRegressor.  
The init function should include the following parameters:
max_iter: Maximum number of iterations;
alpha_1: Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter;
alpha_2: Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter;
lambda_1: Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter;
lambda_2: Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.
The module must contain a fit function and a predict function. The fit function is used to fit the training data, and the predict function is used to predict the labels for the given features.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted labels for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.