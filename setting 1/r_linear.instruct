Given an instruction about a machine learning algorithm, implement the relevant code based on this instruction.
You should implement the algorithm by using Python, Numpy or Scipy from scratch. You can't use any functions or classes from scikit-learn.
You only need to implement the algorithm module, and you don't need to generate test cases.
You should create as many sub-functions or sub-classes as possible to help you implement the entire algorithm.
Just output the code of the algorithm, don't output anything else.

Instruction:

Implement the linear regressor with python, numpy and scipy.  

The main objective of linear regression is to determine the linear relationship between the dependent variable \( y \) and the independent variable(s) \( X \). The simplest form is the simple linear regression, which involves one independent variable. The relationship is modeled by the equation:

\[ y = \beta_0 + \beta_1 x + \epsilon \]

- \( y \) is the dependent variable.
- \( x \) is the independent variable.
- \( \beta_0 \) is the y-intercept of the regression line.
- \( \beta_1 \) is the slope of the regression line.
- \( \epsilon \) is the error term, representing the difference between the observed and predicted values.

### Algorithmic Flow

1. **Data Collection and Preparation**: Gather the dataset containing the dependent and independent variables. Clean and preprocess the data to handle missing values and outliers.

2. **Model Specification**: Define the linear model. For multiple linear regression, the model extends to:

   \[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon \]

3. **Parameter Estimation**: Use the method of least squares to estimate the parameters \( \beta_0, \beta_1, \ldots, \beta_n \). The goal is to minimize the sum of squared residuals (differences between observed and predicted values):

   \[ \text{Minimize } \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 \]

   where \( \hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_n x_{in} \).

4. **Normal Equations**: The least squares estimates can be derived using the normal equation:

   \[ \beta = (X^T X)^{-1} X^T y \]

   Here, \( X \) is the matrix of input features, \( y \) is the vector of observed values, and \( \beta \) is the vector of coefficients.

5. **Model Evaluation**: Assess the model's performance using metrics such as R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics help determine how well the model fits the data.

6. **Prediction**: Use the estimated coefficients to make predictions on new data:

   \[ \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n \]

The module should be named GPTLinearRegressor.  
The module must contain a fit function and a predict function. The fit function is used to fit the training data, and the predict function is used to predict the labels for the given features.  
The fit function accepts X_train, y_train as input and return None where  
X_train: the features of the train data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the train data and d is the dimension.  
y_train: the labels pf the train data,which is a numpy array.  
The predict function accepts X_test as input and return predictions where  
X_test: the features of the test data, which is a numpy array, and the shape of X_train is [N, d]. N is the number of the test data and d is the dimension.  
predctions: the predicted labels for X_test, which is a numpy arrary.  
You should just return the code for the module, don't return anything else.